{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REaV1GblZovs"
      },
      "source": [
        "# Fun with MNIST\n",
        "This lab will teach how to use Keras to build and train neural networks to solve simple computer vision tasks.\n",
        "\n",
        "You will learn about\n",
        "\n",
        "- The MNIST dataset\n",
        "- Softmax regression\n",
        "- Weight decay\n",
        "- Keras' Sequential API and Functional API\n",
        "- Convolutional Neural Networks (CNNs)\n",
        "- Feature embedding (embedding MNIST images in 2D using a CNN)\n",
        "- AutoEncoders\n",
        "- Writing your own data generator\n",
        "\n",
        "**Before we start - remember to set runtime to GPU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qc8RJkIJaxap"
      },
      "source": [
        "## Task 1: Downloading and pre-processing the MNIST dataset\n",
        "The MNIST dataset of handwritten digits is so commonly used that it comes with most deep learning frameworks, including Keras. Let's download the dataset and explore a little bit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "r-_9ZwQkiGQg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 152
        },
        "outputId": "813e3bd9-d70b-4c5d-c6f9-04c61535503f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Examples of test images\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x400 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras import backend as K\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Utility function for showing images\n",
        "def show_imgs(x_test, n=10):\n",
        "    sz = x_test.shape[1]\n",
        "    plt.figure(figsize=(20, 4))\n",
        "    for i in range(n):\n",
        "        ax = plt.subplot(2, n, i+1)\n",
        "        plt.imshow(x_test[i].reshape(sz,sz))\n",
        "        plt.gray()\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "    plt.show()\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "# Pre-process inputs\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "# Convert class indices to one-hot vectors\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "print('Examples of test images')\n",
        "show_imgs(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHvyXrdliPFE"
      },
      "source": [
        "### Questions 1.1\n",
        "1. What is the input shape?\n",
        "2. How many training examples are there?\n",
        "3. How many test examples are there?\n",
        "4. What does `to_categorical` do? (Hint: print `y_test` before and after applying `to_categorial`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OX-mswCi2vY"
      },
      "source": [
        "## Task 2: Softmax regression\n",
        "Now, let's define a tiny Keras model for logistic regression. Mathematically this model outputs a 10-dimensional vector `y` of class probabilities, where\n",
        "\n",
        "\n",
        "```\n",
        "y = softmax(W*x + b)\n",
        "```\n",
        "\n",
        "and\n",
        "- `x` is a 28x28 = 784-dimensional vector corresponding to the input image,\n",
        "- `W` is a 10 x 784 matrix of weights\n",
        "- `b` is a 10-dimensional vector of biases\n",
        "\n",
        "Defining models in Keras is not very intuitive from a mathematical perspective. Here is one way to implement the equation above using Keras' [Sequential API](https://keras.io/getting-started/sequential-model-guide/). A Sequential model is a just a linear stack of layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HSqZ3O4ocxQ-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "4b962df0-cab7-4f41-8ac7-2e1a56c31141"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m7,850\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation (\u001b[38;5;33mActivation\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">7,850</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,850\u001b[0m (30.66 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,850</span> (30.66 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,850\u001b[0m (30.66 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,850</span> (30.66 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Activation\n",
        "\n",
        "keras.utils.set_random_seed(0) # make weight initialization deterministic\n",
        "\n",
        "# Model\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=input_shape)) # input_shape is (28, 28, 1)\n",
        "model.add(Dense(num_classes)) # num_classes is 10\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJBX3bW9kaq8"
      },
      "source": [
        "### Questions 2.1\n",
        "1. What do you think the Flatten() layer does, and why is it necessary to have it here?\n",
        "2. What do you think the Dense() layer does, and why does it have 7850 parameters?\n",
        "3. What do you think Activation() does, and why is its argument set to \"softmax\"?\n",
        "\n",
        "**Tip**\n",
        "\n",
        "To answer questions like the ones above, you could perform simple experiments like the one below:\n",
        "\n",
        "```\n",
        "input_shape = (28, 28, 1)\n",
        "x = tensorflow.random.normal(input_shape)\n",
        "y = keras.layers.Flatten()(x)\n",
        "print(y.shape)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PqpHs6EgXMP"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eABHA5aSmBdy"
      },
      "source": [
        "Now, let's train the model for 10 epochs. We will be using the multi-class version of the cross entropy loss and stochastic gradient descent (SGD). The difference between normal gradient descent and SGD is that normal gradient descent calculates the gradients based on all training examples, whereas SGD approximates the gradient by calculating it on small batches (of size 128 in this example)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qimN5at5djhI"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "epochs = 10\n",
        "\n",
        "# Compile the model before training\n",
        "model.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "# Fit model\n",
        "history = model.fit(x_train, y_train,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            verbose=1,\n",
        "            validation_data=(x_test, y_test))\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Validation loss:', score[0])\n",
        "print('Validation accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoNLXZvAt_Hr"
      },
      "source": [
        "In machine learning it is always a good idea to pick a simple **baseline model** that you can compare your own models to. We will refer to the classifier above as our baseline. You should expect the validation accuracy of our baseline model to be around 90% after training for 10 epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_r7oO_Pm8TK"
      },
      "source": [
        "### Model evaluation\n",
        "For several reasons you always want to monitor how your model performs during training. The simplest way to monitor the training process is by plotting the loss and accuracy curves. Here we are doing it post-training, but there are tools that allow you to monitor the curves in real-time (see for instance [TensorBoard](https://www.tensorflow.org/tensorboard))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gj2-tNJShfjj"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_history(history):\n",
        "  plt.figure(figsize=(20,6))\n",
        "\n",
        "  # summarize history for accuracy\n",
        "  plt.subplot(121)\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'validation'], loc='upper left')\n",
        "\n",
        "  # summarize history for loss\n",
        "  plt.subplot(122)\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'validation'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "show_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDUhPEQcvwyw"
      },
      "source": [
        "Examples of things you should be looking for:\n",
        "\n",
        "- The loss should go down over time.\n",
        " - If it increases, it usually means that your learning rate was set too high.\n",
        " - If it doesn't decrease, it could indicate that your learning rate was set too low.\n",
        "- If the validation loss starts increasing, while the training loss is still decreasing, it means that your model has started overfitting.\n",
        " - Can you explain why?\n",
        "- Simply stated, your model is done training when the loss curves hit a low plateau.\n",
        " - Are we done training in the above example, or do you think we should we be training for more epochs?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8v3SUB2o-Tp"
      },
      "source": [
        "### Task 2.1: Batch size and number of epochs\n",
        "Training on the entire MNIST training data set is guaranteed to work (almost) always. Simply because of the large number of images in the training set. This is boring. So let's make our problem a little more challenging by reducing the number of training examples to just 10 samples from each class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxOTzj5tp46I"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(0) # make it deterministic\n",
        "\n",
        "# create smaller training set\n",
        "digit_indices = np.asarray([np.where(np.argmax(y_train,axis=1) == i)[0][np.random.randint(0,5000,10)] for i in range(num_classes)]).flatten()\n",
        "x_train_small = x_train[digit_indices,:]\n",
        "y_train_small = y_train[digit_indices,:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oymSEIUHMCX"
      },
      "source": [
        "**Note:** Set verbose=1 to print output while training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZVxQgYIIG7Y"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "batch_size = 5 # Cannot use 128 like before, because we only have 100 samples (10 from each class)\n",
        "epochs = 20 # We need more epochs because we have fewer training samples\n",
        "\n",
        "keras.utils.set_random_seed(0)\n",
        "\n",
        "# Model (redefine the model in order to reinitialize the weights to random values)\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=input_shape))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# Compile the model before training\n",
        "model.compile(optimizer=keras.optimizers.SGD(),loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "# Fit model (this will take a little while. Set verbose to 1 if you want to see how training progresses)\n",
        "start_time = time.time()\n",
        "history = model.fit(x_train_small, y_train_small,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            validation_data=(x_test, y_test),\n",
        "            verbose=1,\n",
        "            shuffle=True)\n",
        "print(\"--- training tool %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Validation loss:', score[0])\n",
        "print('Validation accuracy:', score[1])\n",
        "\n",
        "# Plot old vs new loss\n",
        "show_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0d5XHi8xNVL"
      },
      "source": [
        "**Note:** Although we didn't change the underlying equation of our baseline model (`y = softmax(Wx+b)`), we did change the nature of the data on which the model was trained, and we also lowered the batch size and increased the number of epochs. So this is considered a **new model** that we could compare to our baseline model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkmF52862hxn"
      },
      "source": [
        "### Questions 2.2\n",
        "1. Our new model performs worse than the baseline model. Why?\n",
        "2. This puzzle is supposed to make you speculate. You are not required to get the answers right at this stage of the course. Temporarily set the number of epochs to 10 to make experimentation faster. How does training behave when we set the batch size to the values listed below? Compare the loss curves and the training times and see if you can figure out why they differ the way they do. Use these batch sizes:\n",
        " - 1 (lowest possible)\n",
        " - 5 (low)\n",
        " - 20 (medium)\n",
        " - 100 (highest possible)\n",
        "3. Above, we changed the batch size and the number of epochs. Are these *hyperparameters* or *learnable parameters*? Btw., what are the learnable parameters of our model (``y = softmax(W*x + b)``)?\n",
        "4. Which criteria would you pick to determine the \"optimal\" combination of batch size and number of epochs?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmXf7cvR_8Vm"
      },
      "source": [
        "**IMPORTANT:** Before continuing to the next task, retrain the model above using a batch size of 5 and number of epochs set to 20."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jypf_eK3Iopc"
      },
      "source": [
        "### Task 2.2: Finding a better learning rate\n",
        "The learning rate is another hyperparameter that we can tweak.\n",
        "\n",
        "Training with the default learning rate (which is 0.01) is rather slow. Your task is to find a better learning rate that makes the model converge faster, without comprimising the model's accuracy on the validation set. With a proper learning rate you should be able to achieve 74-75% accuracy in just 10 epochs (instead of 20 as above).\n",
        "\n",
        "You can adjust the learning rate by setting the `learning_rate` argument of keras.optimizer.SGD:\n",
        "\n",
        "```\n",
        "keras.optimizers.SGD(learning_rate=0.01)\n",
        "```\n",
        "\n",
        "**Optional bonus questions:** What happens if you set the learning rate way too low (e.g. 0.0001)?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTIMPPIXnmps"
      },
      "source": [
        "### Task 2.3: Displaying the learned weights\n",
        "In the case of softmax regression there is a very intuitive interpretation of the learned weights of the coefficient matrix `W`, as you will see below.\n",
        "\n",
        "First, your task is to extract the weights of the coefficient matrix from the model (i.e. the `Dense` layer) and display each row as an image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTDJCvLqn_3n"
      },
      "outputs": [],
      "source": [
        "W = # Your code goes here\n",
        "W = W.reshape((28,28,10)) # there are 10 classes and one 28x28 weight image per class\n",
        "plt.figure(figsize=(20,4))\n",
        "for i in range(10):\n",
        "  ax = plt.subplot(2,5,i+1)\n",
        "  plt.imshow(W[:,:,i])\n",
        "  ax.get_xaxis().set_visible(False)\n",
        "  ax.get_yaxis().set_visible(False)\n",
        "  plt.gray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCMuJthmJCoj"
      },
      "source": [
        "### Questions 2.3\n",
        "1. How do we interpret the weights?\n",
        "2. In some regions the weights are very noisy. What could be the consequences of that?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_HLWIZ7MUms"
      },
      "source": [
        "### Task 2.4: Weight decay (or L2 regularization)\n",
        "With only 10 observations per class in our training data set, it is very likely that our model overfits the training data. This leads to poor generalization (i.e., the model doesn't work that well on unseen data).\n",
        "\n",
        "One way to address overfitting is by means of regularization. The best kind of regularization is \"adding more data\" (of course). This is the reason that our baseline model performs better than the current model.\n",
        "\n",
        "One possible regularization strategy is to use *weight decay*. So let's modify the loss function of the model by adding an L2 regularization term. The regularization term is added using an extra parameter to the Dense layer.\n",
        "\n",
        "Please note that the weight of the penalty term (`lamda`) has been set rather high in the example below. As a consequence, you will actually see a small decrease in validation accuracy. But as you will see later, weight decay has dramatic effect on the learned weights (`W`). By the way, `lambda` is yet another hyperparameter that we could tweak to make our model perform better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiY_I0FKLmBv"
      },
      "outputs": [],
      "source": [
        "batch_size = 5\n",
        "epochs = 20\n",
        "\n",
        "keras.utils.set_random_seed(0)\n",
        "\n",
        "# lamda is the weight of the L2 penalty term\n",
        "lamda = 0.1\n",
        "L2_regularizer = keras.regularizers.l2(lamda)\n",
        "\n",
        "# Model\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=input_shape))\n",
        "model.add(Dense(num_classes,\n",
        "                activation='softmax',\n",
        "                kernel_regularizer=L2_regularizer))\n",
        "\n",
        "# Training\n",
        "model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.1),loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "history_reg = model.fit(x_train_small, y_train_small,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            verbose=1,\n",
        "            validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluation\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Validation loss:', score[0])\n",
        "print('Validation accuracy:', score[1])\n",
        "\n",
        "# Plot loss\n",
        "print(\"Loss curves with regularization\")\n",
        "show_history(history_reg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OigocSPpfuo"
      },
      "source": [
        "**Now comes the interesting part:** Try once more to display the weights - this time of the regularized model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ry1Am1ddFV9"
      },
      "outputs": [],
      "source": [
        "#W = # Your code goes here\n",
        "W = W.reshape((28,28,10))\n",
        "plt.figure(figsize=(20,4))\n",
        "for i in range(10):\n",
        "  ax = plt.subplot(2,5,i+1)\n",
        "  plt.imshow(W[:,:,i])\n",
        "  ax.get_xaxis().set_visible(False)\n",
        "  ax.get_yaxis().set_visible(False)\n",
        "  plt.gray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rivkoCuppm3"
      },
      "source": [
        "### Questions 2.4\n",
        "1. How do the weights of the regularized model differ from the weights of the non-regularized model?\n",
        "2. Can you explain why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y2gg6tWesR1"
      },
      "source": [
        "### Functional API instead of Sequential API\n",
        "The models above have been specified using Keras' [Sequential API](https://keras.io/getting-started/sequential-model-guide/). Keras also allows you to specify models using the [Functional API](https://keras.io/getting-started/functional-api-guide/). The Keras functional API is the way to go for defining complex models, such as multi-output models, directed acyclic graphs, or models with shared layers.\n",
        "\n",
        "Here is how to set up the (baseline) softmax regression model using the functional API:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZ3KOeSjpoTJ"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "\n",
        "keras.utils.set_random_seed(0)\n",
        "\n",
        "# This returns a tensor\n",
        "inputs = Input(shape=input_shape)\n",
        "\n",
        "# A layer instance is callable on a tensor, and returns a tensor\n",
        "x = Flatten()(inputs)\n",
        "x = Dense(num_classes)(x)\n",
        "predictions = Activation('softmax')(x)\n",
        "\n",
        "# This creates a model that includes the Input layer and the prediction layer\n",
        "model = Model(inputs=inputs, outputs=predictions)\n",
        "\n",
        "# Training\n",
        "model.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=128,\n",
        "          epochs=10,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluation\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Validation loss:', score[0])\n",
        "print('Validation accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHf86VZYlrAB"
      },
      "source": [
        "## Task 3: Our first CNN\n",
        "It's time to move on and build our first CNN.\n",
        "\n",
        "Here is a simple example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvHAmMda0uNE"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Dropout\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "\n",
        "keras.utils.set_random_seed(0)\n",
        "\n",
        "inputs = Input(shape=(28, 28, 1))\n",
        "\n",
        "# Encoder (convolutional base)\n",
        "x = Conv2D(filters=8, kernel_size=(3, 3), activation='relu')(inputs)\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "x = Conv2D(filters=16, kernel_size=(3, 3), activation='relu')(x)\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "x = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(x)\n",
        "encoded = Flatten()(x)\n",
        "\n",
        "# Decoder (2 fully connected layers)\n",
        "x = Dense(units=64, activation='relu')(encoded)\n",
        "x = Dropout(rate=0.5)(x)\n",
        "predictions = Dense(units=num_classes,activation='softmax')(x)\n",
        "\n",
        "# This creates a callable model that includes the Input layer and the prediction layer\n",
        "model = Model(inputs=inputs, outputs=predictions)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNATzsjAnWCi"
      },
      "source": [
        "### Questions 3.1\n",
        "1. How many layers does this CNN have?\n",
        "2. How many convolution filters are there in the first convolution layer, and what is the width and height of the filters?\n",
        "3. What does the MaxPooling2D layer do?\n",
        "4. What does the Dropout layer do?\n",
        "5. What is the shape of the output of the last convolution layer (i.e., just before the flatten layer)\n",
        "\n",
        "**Recall**\n",
        "\n",
        "You can make small experiments like this one\n",
        "\n",
        "```\n",
        "input_shape = (1, 28, 28, 1) # Dimensions are (batch_size, height, width, num_channels)\n",
        "x = tensorflow.random.normal(input_shape)\n",
        "y = keras.layers.Conv2D(filters=2, kernel_size=3, activation='relu', padding=\"same\", input_shape=input_shape[1:])(x)\n",
        "print(y.shape)\n",
        "```\n",
        "\n",
        "**Optional bonus task:** See if you can figure out a way to display the filters of the first convolution layer as images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zgpNVDP2nKi"
      },
      "source": [
        "For the record, the same model can also be defined using the sequential API:\n",
        "\n",
        "\n",
        "```\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Conv2D ![alt text](https://), MaxPooling2D\n",
        "\n",
        "input_img = Input(shape=(28, 28, 1))\n",
        "model = Sequential()\n",
        "model.add(Conv2D(8,\n",
        "                 kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(16,\n",
        "                 kernel_size=(3, 3),\n",
        "                 activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(32,\n",
        "                 kernel_size=(3, 3),\n",
        "                 activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64,\n",
        "                activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44dU5LYWrMXY"
      },
      "source": [
        "### Training\n",
        "Let's train our model on the small dataset and evaluate it on the validation set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tB02sLW7mbX6"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.1),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train_small, y_train_small,\n",
        "                    batch_size=10,\n",
        "                    epochs=30,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Validation loss:', score[0])\n",
        "print('Valdiation accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_1w3A-jr0od"
      },
      "outputs": [],
      "source": [
        "show_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnRuBQORVrQy"
      },
      "source": [
        "Notice how this model is able to achieve a higher validation accuracy on the small training dataset (before we got 74.4%, now we have 80%). It does come at an expense, however; the CNN model needs to train for longer, having to do with the fact that it has more layers and more learnable parameters.\n",
        "\n",
        "**Note:** If you train the above CNN model on the full MNIST dataset, you can expect to get at least 97-98% validation accuracy. That is quite an improvement over our baseline model (softmax regression), which got 89%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOxQ0LletyuB"
      },
      "source": [
        "### Task 3.1 Encoding the MNIST dataset in 2 dimensions with a CNN\n",
        "In the above model the input shape to the Flatten layer  is 3x3x32, which is then flattened to a 288-dimensional vector, corresponding to the variable named `encoded`.\n",
        "\n",
        "Your task is to modify the network such that variable `encoded` has dimensionality 2 instead of 288.\n",
        "\n",
        "*Hint:* You could insert an extra layer before the Flatten layer that reduces the 3x3x32 input tensor to a 1x1x2 tensor. There are several solutions. It might be a good idea to add an activation function to the layer that you add. I used `activation='tanh'`. I will explain in the solution why this is a good idea."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nF0XJUxJ4Qpj"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Dropout\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "keras.utils.set_random_seed(0)\n",
        "\n",
        "inputs = Input(shape=(28, 28, 1))\n",
        "\n",
        "# Encoder (convolutional base)\n",
        "x = Conv2D(8, kernel_size=(3, 3), activation='relu')(inputs)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Conv2D(16, kernel_size=(3, 3), activation='relu')(x)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Conv2D(32, kernel_size=(3, 3), activation='relu')(x)\n",
        "x = # Your code goes here\n",
        "encoded = Flatten()(x)\n",
        "\n",
        "# Decoder (2 fully connected layers)\n",
        "x = Dense(64, activation='relu')(encoded)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(num_classes,activation='softmax')(x)\n",
        "\n",
        "# This creates a callable model that includes the Input layer and the prediction layer\n",
        "model = Model(inputs=inputs, outputs=predictions)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69KNBxBEv53g"
      },
      "source": [
        "### Training\n",
        "This time we train the model on the full dataset, because compressing the images to 2 dimensions is a difficult task that requires more training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyDbES-izBol"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=100,\n",
        "          epochs=10,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HrN8wbHwA0q"
      },
      "source": [
        "### Task 3.2\n",
        "Run all test examples (`x_test`) through your model and for each example extract the 2-dimensional vector output of the Flatten layer (variable named `encoded`). To make predictions on a batch of images, you can do like this: ``out = model_encoded.predict(batch)``.\n",
        "\n",
        "Then plot those vectors in a 2D plot, where each class gets its own color.\n",
        "\n",
        "You might find this code useful:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGUoaLHVwn5T"
      },
      "outputs": [],
      "source": [
        "model_encoded = Model(inputs=model.input, outputs=encoded)\n",
        "\n",
        "# Plot 10 dots with 10 different colors\n",
        "for i in range(10):\n",
        "  plt.plot(i,i,'.C'+str(i))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iR1BvO15aCQ"
      },
      "source": [
        "### Comment\n",
        "The point of this little exercise is really to illustrate the power of CNNs as image encoders. Think of what we have just accompplished: We have embedded 28x28=784 dimensional images/vectors down to just 2 dimensions, and we can still distinguish the 10 classes from each other! This is a really powerful concept for visualizing high-dimensional data in 2D."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YJnv8WI7b4B"
      },
      "source": [
        "## Task 4: Convolutional Autoencoder\n",
        "Autoencoders are special types of neural networks that map the input X to the same output (namely X). So the autoencoder (AE) is an identity function:\n",
        "\n",
        "```\n",
        "X = AE(X)\n",
        "```\n",
        "\n",
        "So what's the point? The point is that the autoencoder compresses the image down to a low-dimensional representation, which can be decoded again to reconstruct the original input image. This has many useful applications, such as data compression and representation learning. Only the important information is stored in the low-dimensional representation.\n",
        "\n",
        "The autoencoder consists of a trained encoder (E) and a trained decoder (D):\n",
        "\n",
        "```\n",
        "X = AE(X) = D(E(X))\n",
        "```\n",
        "\n",
        "It is typically (but not always) the encoding E(X) that we are interested in.\n",
        "\n",
        "Note that the autoecoder does not need the class labels to train. So it is an **unsupervised** machine learning technique.\n",
        "\n",
        "Here is an example of a convolutional autoencoder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNUtSwB-1thF"
      },
      "outputs": [],
      "source": [
        "from keras.layers import UpSampling2D\n",
        "\n",
        "keras.utils.set_random_seed(0)\n",
        "inputs = Input(shape=(28, 28, 1))\n",
        "\n",
        "# Encoder (convolutional base)\n",
        "x = Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same')(inputs)\n",
        "x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = Conv2D(8, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = Conv2D(8, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
        "print(\"shape of encoded\", encoded.shape)\n",
        "\n",
        "# Decoder (upsamling)\n",
        "x = Conv2D(8, kernel_size=(3, 3), activation='relu', padding='same')(encoded)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "x = Conv2D(8, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "x = Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "decoded = Conv2D(1, kernel_size=(5, 5), padding='valid')(x)\n",
        "print(\"shape of decoded\", decoded.shape)\n",
        "\n",
        "autoencoder = Model(inputs, decoded)\n",
        "autoencoder.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nk0IV2fFAfuw"
      },
      "source": [
        "### Questions 4.1\n",
        "1. What is the shape of the encoded image?\n",
        "2. Why are we not flattening (i.e., vectorizing) the encoded image like we did in the CNN classifier above?\n",
        "3. What does UpSampling2D do?\n",
        "4. Why do you think upsampling is followed by a convolution? Hint: It has to do with the way we are upsampling...\n",
        "5. See if you can figure out what `padding` means. What is the difference between using `padding='same'` and `padding='valid'`?\n",
        "\n",
        "**Again** remember that you can carry out small experiments to get the answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKXQlf8tBnmc"
      },
      "source": [
        "### Training\n",
        "Let's train the autoencoder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vw_OqAHD5k1-"
      },
      "outputs": [],
      "source": [
        "autoencoder.compile(optimizer=keras.optimizers.SGD(learning_rate=0.01, momentum=0.9), loss='mse')\n",
        "autoencoder.fit(x_train, x_train, epochs=10, batch_size=100,\n",
        "               shuffle=True, validation_data=(x_test, x_test), verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfCKH2rMgSOi"
      },
      "source": [
        "And show some test results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edNyzCMW6PaS"
      },
      "outputs": [],
      "source": [
        "decoded_imgs = autoencoder.predict(x_test)\n",
        "print(\"input (upper row)\")\n",
        "show_imgs(x_test)\n",
        "print(\"decoded (bottom row)\")\n",
        "show_imgs(decoded_imgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LAsmAerCSLh"
      },
      "source": [
        "### Improving the autoencoder\n",
        "This is okay, but not great. Let's use binary cross entropy (BCE) loss instead of mean squared error (MSE) loss. Using BCE assumes that the inputs are a probabilities, hence the sigmoid below to squash the values in between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJJhiPNeCiAg"
      },
      "outputs": [],
      "source": [
        "decoded_sigmoid = Activation('sigmoid')(decoded) # decoded is the output of the first autoencoder\n",
        "autoencoder2 = Model(inputs, decoded_sigmoid)\n",
        "autoencoder2.compile(optimizer=keras.optimizers.SGD(learning_rate=0.01, momentum=0.9), loss='binary_crossentropy')\n",
        "autoencoder2.fit(x_train, x_train, epochs=10, batch_size=100,\n",
        "               shuffle=True, validation_data=(x_test, x_test), verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyRb-DNXDBBM"
      },
      "outputs": [],
      "source": [
        "decoded_imgs = autoencoder2.predict(x_test)\n",
        "print(\"input (upper row)\")\n",
        "show_imgs(x_test)\n",
        "print(\"decoded (bottom row)\")\n",
        "show_imgs(decoded_imgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYNuOKadDk_P"
      },
      "source": [
        "### Questions 4.2\n",
        "1. What is the difference between 'mse' loss and 'binary_crossentropy' loss?\n",
        "2. Can you explain why 'binary_crossentropy' works better?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEU1ZDqg6bk9"
      },
      "source": [
        "## Task 5: Denoising Autoencoder\n",
        "Autoencoders can get really advanced, like [Variational Autoencoders](https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf). A slightly less complicated, yet powerful autoecoder variant is the [Denoising Autoencoder](https://www.cs.toronto.edu/~larocheh/publications/icml-2008-denoising-autoencoders.pdf).\n",
        "\n",
        "As stated above autoencoders have many useful applications. One of these is *noise reduction*. The underlying idea is very simple: Add random noise to the input X, and teach the autoencoder to remove the noise. That is, the autoencoder should learn the mapping:\n",
        "\n",
        "```\n",
        "X = AE(X + noise)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnzL99s2IYtR"
      },
      "source": [
        "### Task 5.1\n",
        "Create two new data sets based on x_train and x_test, where you have added noise such that\n",
        "\n",
        "```\n",
        "x_train_noisy = x_train + noise\n",
        "x_test_noisy = x_test + noise\n",
        "```\n",
        "\n",
        "You may want to look at numpy functions like [np.random.normal](https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html) and [np.clip](https://numpy.org/doc/stable/reference/generated/numpy.clip.html).\n",
        "\n",
        "**Your tasks:**\n",
        "\n",
        "1. Does ``autoencoder2`` work on the noisy images, ``x_test_noisy``? (test it!)\n",
        "2. Retrain ``autoencoder2`` on the noisy images (input = ``x_train_noisy`` and output = ``x_train``). What do you observe?\n",
        "\n",
        "How much noise can you add before the autoencoder breaks down (fails to remove the noise)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wD06QF9EwDbp"
      },
      "outputs": [],
      "source": [
        "#Your code goes here\n",
        "#x_train_noisy = ???\n",
        "#x_test_noisy = ???"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-16HefOKrJF"
      },
      "source": [
        "Let's first verify that the existing autoencoder doesn't work well on noisy input images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vH7PBypK2WQ"
      },
      "outputs": [],
      "source": [
        "# denoising\n",
        "print(\"denoising\")\n",
        "decoded_imgs = autoencoder2.predict(x_test_noisy)\n",
        "print(\"input (upper row)\")\n",
        "show_imgs(x_test_noisy)\n",
        "print(\"decoded (bottom row)\")\n",
        "show_imgs(decoded_imgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-ENwW7aLL36"
      },
      "source": [
        "Doesn't look that good. Let's see if we can teach the model to remove the noise by showing it training images that contain noise. Instead of training from scratch, we will *fine-tune* the model that has already been trained:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YH5iYSws6nI0"
      },
      "outputs": [],
      "source": [
        "# it takes more epochs to converge\n",
        "autoencoder2.fit(x_train_noisy, x_train, epochs=10, batch_size=100,\n",
        "                shuffle=True, validation_data=(x_test_noisy, x_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynvpaB666wlm"
      },
      "outputs": [],
      "source": [
        "# denoising\n",
        "print(\"denoising\")\n",
        "decoded_imgs = autoencoder2.predict(x_test_noisy)\n",
        "print(\"input (upper row)\")\n",
        "show_imgs(x_test_noisy)\n",
        "print(\"decoded (bottom row)\")\n",
        "show_imgs(decoded_imgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Zoir9fpLjpK"
      },
      "source": [
        "Pretty cool, right?\n",
        "\n",
        "Of cause the updated autoencoder still works on the noise-free images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbNotnbQLnqP"
      },
      "outputs": [],
      "source": [
        "# what if we feed the original noise-free test images?\n",
        "decoded_imgs = autoencoder2.predict(x_test)\n",
        "print(\"\\nof course, it works with original noise-less images\")\n",
        "print(\"input (upper row)\")\n",
        "show_imgs(x_test)\n",
        "print(\"decoded (bottom row)\")\n",
        "show_imgs(decoded_imgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjUB_cm4yRj6"
      },
      "source": [
        "## Task 6: Image regression\n",
        "Recall that in a regression problems the output of the model is one or more scalar values, rather than class labels. Both the autoencoder and super resolution network are examples of regression models. Another example of image regression is [facial landmark prediction](https://medium.com/@rishiswethan.c.r/emotion-detection-using-facial-landmarks-and-deep-learning-b7f54fe551bf), which can be used for emotion recognition.\n",
        "\n",
        "In this task we will estimate the rotation angle of rotated MNIST images (but it could just as well have been estimating pixel coordinates of facial landmarks).\n",
        "\n",
        "As a first step, we need an image generator that generates batches of randomly rotated images, along with the target rotation angles that the model should learn to predict. This code was modified from https://d4nst.github.io/2017/01/12/image-orientation/\n",
        "\n",
        "**Optional bonus task:** Writing your own custom data generators is a common task in deep learning. Once you have completed the main task, consider going through the code of  `RotNetDataGenerator` and see if you can figure out how it works (notice that its base class is [`https://keras.io/api/utils/python_utils#pydataset-class`](https://keras.io/api/utils/python_utils#pydataset-class))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAhmL_jj_5m-"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "class RotNetDataGenerator(keras.utils.PyDataset):\n",
        "\n",
        "    def __init__(self, input, batch_size=64):\n",
        "\n",
        "        self.images = input\n",
        "        self.batch_size = batch_size\n",
        "        self.input_shape = self.images.shape[1:]\n",
        "\n",
        "        # Add dimension because the images are greyscale\n",
        "        if len(self.input_shape) == 2:\n",
        "            self.input_shape = self.input_shape + (1,)\n",
        "        self.num_samples = self.images.shape[0]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # create array to hold the images\n",
        "        batch_x = np.zeros((self.batch_size,) + self.input_shape, dtype='float32')\n",
        "        # create array to hold the labels\n",
        "        batch_y = np.zeros(self.batch_size, dtype='float32')\n",
        "\n",
        "        indices = [i for i in range(self.batch_size*idx, self.batch_size*(idx+1))]\n",
        "\n",
        "        # iterate through the current batch\n",
        "        for i, j in enumerate(indices):\n",
        "\n",
        "            image = self.images[j].squeeze()\n",
        "\n",
        "            # get a random angle\n",
        "            rotation_angle = np.random.randint(-30,30)\n",
        "\n",
        "            # rotate the image\n",
        "            rows,cols = image.shape\n",
        "            M = cv2.getRotationMatrix2D(((cols-1)/2.0,(rows-1)/2.0),rotation_angle,1)\n",
        "            rotated_image = cv2.warpAffine(image,M,(cols,rows))\n",
        "\n",
        "            # add dimension to account for the channels if the image is greyscale\n",
        "            if rotated_image.ndim == 2:\n",
        "                rotated_image = np.expand_dims(rotated_image, axis=2)\n",
        "\n",
        "            # store the image and label in their corresponding batches\n",
        "            batch_x[i] = rotated_image\n",
        "            batch_y[i] = rotation_angle\n",
        "\n",
        "        return batch_x, batch_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBRKLikE37WE"
      },
      "source": [
        "### Test the generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJSiRdxcBcEQ"
      },
      "outputs": [],
      "source": [
        "# Instantiate\n",
        "datagen = RotNetDataGenerator(\n",
        "        x_train,\n",
        "        batch_size=32,\n",
        "    )\n",
        "\n",
        "# Generate batch\n",
        "rotated_images, angles = datagen.__getitem__(0)\n",
        "\n",
        "# Display\n",
        "print(\"Images (before rotation)\")\n",
        "show_imgs(x_train)\n",
        "print(\"Images after random rotation\")\n",
        "show_imgs(rotated_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rWkMWvh9hNt"
      },
      "source": [
        "### Task 6.1\n",
        "Make a small CNN that takes as input an 28x28x1 image and outputs a single scalar value (the rotation angle).\n",
        "\n",
        "The last layer of your network should be\n",
        "\n",
        "```\n",
        "angle = Dense(1)(x)\n",
        "```\n",
        "\n",
        "Note that this is a dense layer **without** any activation function, hence the output of this layer is simply `angle = W*x + b`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3EMvU5N94L3"
      },
      "outputs": [],
      "source": [
        "inputs = Input(shape=(28, 28, 1))\n",
        "\n",
        "# Encoder (convolutional base)\n",
        "# Your codes goes here\n",
        "\n",
        "# Decoder (predict angle)\n",
        "x = # Your code goes here\n",
        "angle = Dense(1)(x)\n",
        "\n",
        "angle_estimator = Model(input=inputs, output=angle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9eeVPFI-tVq"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bW2RzBPJG0P-"
      },
      "outputs": [],
      "source": [
        "angle_estimator.compile(optimizer='rmsprop',loss='mse')\n",
        "\n",
        "# training loop\n",
        "angle_estimator.fit(\n",
        "    RotNetDataGenerator(\n",
        "        x_train,\n",
        "        batch_size=100\n",
        "    ),\n",
        "    epochs=50,\n",
        "    validation_data=RotNetDataGenerator(\n",
        "        x_test,\n",
        "        batch_size=100\n",
        "    ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98or9_7_6YNu"
      },
      "source": [
        "### Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYmyHlZ5MEKH"
      },
      "outputs": [],
      "source": [
        "# Set up generator\n",
        "datagen = RotNetDataGenerator(\n",
        "        x_test,\n",
        "        batch_size=32\n",
        "    )\n",
        "\n",
        "# Generate test images\n",
        "rotated_images, angles = datagen.__getitem__(0)\n",
        "print(\"Test images before rotation\")\n",
        "show_imgs(x_test)\n",
        "print(\"Test images after rotation\")\n",
        "show_imgs(rotated_images)\n",
        "\n",
        "# Predict angles\n",
        "angles_pred = angle_estimator.predict(rotated_images)\n",
        "\n",
        "# Plot angles\n",
        "print('Predicted vs. true rotation angles')\n",
        "plt.plot(angles)\n",
        "plt.plot(angles_pred)\n",
        "plt.legend(['True','Predicted']);\n",
        "plt.xlabel('Test image')\n",
        "plt.ylabel('Rotation angle')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqtngpfbAaM8"
      },
      "source": [
        "**HELP**: The predicted angles should match the true angles reasonably well. If your model fails to predict the angles, it could be because the model is underfitting. This indicates that the capacity of the model is too low. To increase capacity, add more connections in dense layers and/or more output maps in convolutional layers (I will explain this in the class)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLrZfojj8Cd7"
      },
      "source": [
        "### De-rotate images\n",
        "Now that we have estimated the rotation angles, let's de-rotate the images back to their original alignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H78jSdQoNC5-"
      },
      "outputs": [],
      "source": [
        "de_rotated_images = np.zeros(rotated_images.shape)\n",
        "\n",
        "for i in range(rotated_images.shape[0]):\n",
        "  image = rotated_images[i,:,:,:].squeeze()\n",
        "\n",
        "  # get predicted angle\n",
        "  rotation_angle = -angles_pred[i]\n",
        "\n",
        "  # rotate the image\n",
        "  rows,cols = image.shape\n",
        "  M = cv2.getRotationMatrix2D(((cols-1)/2.0,(rows-1)/2.0),float(rotation_angle),1)\n",
        "  de_rotated_image = cv2.warpAffine(image,M,(cols,rows))\n",
        "\n",
        "  de_rotated_images[i,:,:,0] = de_rotated_image\n",
        "\n",
        "print('Images before rotation (ground truth)')\n",
        "show_imgs(x_test)\n",
        "print('Images after rotation (to be de-rotated)')\n",
        "show_imgs(rotated_images)\n",
        "print('De-rotated images (should match ground truth)')\n",
        "show_imgs(de_rotated_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLphCiQk9CZR"
      },
      "source": [
        "## Bonus tasks\n",
        "In case you completed all of the above and you still have time left, here are some ideas for extra tasks:\n",
        "\n",
        "### Super resolution\n",
        "The convolutional autoencoder is a network that maps an image to another image. There are other types of these image-to-image networks.\n",
        "\n",
        "One example is a super resolution network. This is pretty much an autoencoder, except that the input image has lower spatial resolution than the output image. Super resolution networks learn to increase the spatial of the input image.\n",
        "\n",
        "Your task is to modify the autoencoder such that it takes an 14x14x1 image as input and transforms it to a 28x28x1 image. Specifically, the training and test inputs should be"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuCrfo9pTnx6"
      },
      "outputs": [],
      "source": [
        "# Low resolution images (pick every other pixel)\n",
        "x_train_lowres = x_train[:,::2,::2,:] # 14x14x1\n",
        "x_test_lowres = x_test[:,::2,::2,:] # 14x14x1\n",
        "\n",
        "# Show example\n",
        "print(\"Input low resolution images\")\n",
        "show_imgs(x_train_lowres)\n",
        "print(\"Output high resolution images (target)\")\n",
        "show_imgs(x_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xq6WpaXP94oF"
      },
      "source": [
        "### Autoencoder based on U-net\n",
        "One disadvantage of the autoencoder implemented above is that the fine image details are lost in the encoder, making them impossible to recover for the decoder. This results in output images that are more blurry than the input images.\n",
        "\n",
        "One way to recover the fine details is through so-called *skip connections*, first introduced in the U-net architecture. See if you can figure out what it is and see of you can implement an autoencoder with skip connections.\n",
        "\n",
        "![alt text](https://miro.medium.com/max/720/1*OkUrpDD6I0FpugA_bbYBJQ.png)\n",
        "\n",
        "**Help:** Skip connections connect each layer of the encoder with a corresponding layer in the decoder (illustrated by the horizontal gray arrows in the figure above). There are many ways to implement skip connections. It is basically all about getting the shapes of the encoder and decoder to match.\n",
        "\n",
        "**Useful links:**\n",
        "\n",
        "- https://arxiv.org/pdf/1505.04597.pdf\n",
        "- https://towardsdatascience.com/understanding-semantic-segmentation-with-unet-6be4f42d4b47\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1X_ruY99sup"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}